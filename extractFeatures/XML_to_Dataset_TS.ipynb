{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2dfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: mdn769@usask.ca\n",
    "\n",
    "'''\n",
    "This will first generate the commit pattern files after reading the XML file generated by srcML tool. \n",
    "Then it will convert those commit patterns to GML graph representation. \n",
    "'''\n",
    "\"\"\"\n",
    "import xml.etree.ElementTree as ET\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009f9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()\n",
    "patterns=[]\n",
    "def print_path(n, val=\"\"):   \n",
    "    str_val=str(n.tag).replace(\"{http://www.srcML.org/srcML/src}\",\"\").replace(\"{http://www.srcML.org/srcML/cpp}\",\"\")\n",
    "    val+=\"-\"+str_val\n",
    "    length=len(n)\n",
    "    #print(length, str_val)\n",
    "    if(length>0):\n",
    "        for x in n:\n",
    "            print_path(x, val)\n",
    "    elif(len(val)>0): \n",
    "        newVal = val.replace(\"decl_stmt-decl-\", \"decl-\").replace(\n",
    "            \"argument_list-argument-\", \"argument-\").replace(\n",
    "            \"expr_stmt-expr-\", \"expr-\").replace(\n",
    "            \"parameter_list-parameter-\", \"parameter-\").replace(\n",
    "            \"block-block_content-\", \"block-\").replace(\n",
    "            \"if_stmt-if-\", \"if-\").replace(\n",
    "            \"-empty_stmt\", \"\")\n",
    "        \n",
    "        patterns.append(newVal) #remove unwanted patterns\n",
    "        \n",
    "        \n",
    "def generate_graph(patterns):\n",
    "    G.clear()\n",
    "    node_list=[]    \n",
    "    for cp in patterns:\n",
    "        path=cp[1:].split('-') #First position has unnecessary '-'\n",
    "        for n in path:\n",
    "            if(n not in node_list):\n",
    "                G.add_node(n) #Add unique nodes. \n",
    "        for i in range(0, len(path)-1):\n",
    "            #print(path[i], \"-->\", path[i+1])\n",
    "            if(G.has_edge(path[i], path[i+1]) == False):\n",
    "                G.add_edge(path[i], path[i+1])     \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bfaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/mdn769/BugFixStatistics/SCGrunable/runable/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8f1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for: (2604/2605), ffef22cae224fd6c575e64a73e5017e5855c9aff.cpp.xml\n"
     ]
    }
   ],
   "source": [
    "''' This will find a list of unique token sequences (TS Features) from all the files in this subject system. Each item of the list will be\n",
    "the feature name of the dataset to be prepared. \n",
    "'''\n",
    "list_ts_features = []\n",
    "count = 0\n",
    "inPath = file_path+ \"source_xml/qtbase/added/\"\n",
    "totalFiles = len(os.listdir(inPath))\n",
    "for file in os.listdir(inPath):   \n",
    "#     if(count == 5):\n",
    "#         break\n",
    "    #print(inPath+file)\n",
    "    file_tokens = []\n",
    "    if(file[-4:]==\".xml\"):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Working for: (\"+str(count)+\"/\"+str(totalFiles)+\")\" + \", \"+ file)    \n",
    "        try:\n",
    "            tree = ET.parse(inPath+file)\n",
    "            root = tree.getroot()\n",
    "            patterns=[]\n",
    "            for n in root:\n",
    "                print_path(n)\n",
    "            \n",
    "            for p in patterns:\n",
    "                for t in p.split('-'):\n",
    "                    if(len(t)>0):\n",
    "                        file_tokens.append(t)\n",
    "                        \n",
    "            for ngram in [2, 3]: #n-gram = 2 & 3\n",
    "                temp=zip(*[file_tokens[i:] for i in range(0, ngram)]) \n",
    "                ans=[' '.join(ngram) for ngram in temp]\n",
    "            \n",
    "            for a in ans:\n",
    "                if(a not in list_ts_features):\n",
    "                    list_ts_features.append(a)\n",
    "                    \n",
    "        except: not_worked += ss+\", \"+ctype+\", \"+file+\"\\n\"\n",
    "\n",
    "    count+=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f252a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6461 ['decl type specifier', 'type specifier decl', 'specifier decl type', 'decl type name', 'type name decl', 'name decl name', 'decl name decl', 'name decl init', 'decl init expr', 'init expr call']\n"
     ]
    }
   ],
   "source": [
    "print(len(list_ts_features), list_ts_features[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eba42ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter(list_ts_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d3a6f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working for: (2604/2605)qtbase, added, ffef22cae224fd6c575e64a73e5017e5855c9aff.cpp.xml\n"
     ]
    }
   ],
   "source": [
    "''' Prepare the dataset. For each commit, list_pattern is the list of feature values. \n",
    "'''\n",
    "dataset = [[\"commit_id\"] + list_ts_features] # prepare the top row of the dataset. \n",
    "#subject_systems=[\"bitcoin\", \"jenkins\", \"litecoin\", \"lucene\", \"mongo\", \"oozie\"]\n",
    "subject_systems=[\"qtbase\"]\n",
    "#commit_type=[\"added\", \"deleted\"]\n",
    "commit_type=[\"added\"]\n",
    "not_worked=\"\"\n",
    "# fileContent = open(\"ssNames.txt\", \"r\", encoding='utf-8').read()\n",
    "# #print(\"Length of File Content\", len(fileContent))\n",
    "# ssNames = fileContent.split('\\n')\n",
    "\n",
    "for ss in subject_systems:\n",
    "    if(len(ss.strip()) == 0): #Ignore the last empty line....\n",
    "        break\n",
    "    for ctype in commit_type:\n",
    "        inPath = file_path+ \"source_xml/\"+ss+\"/\"+ctype+\"/\"\n",
    "        #outPath = \"xml_gml/\"+ss+\"/\"+ctype+\"/\"\n",
    "        \n",
    "        count = 0\n",
    "        totalFiles = len(os.listdir(inPath))\n",
    "        for file in os.listdir(inPath):   \n",
    "            #if(count == 10):\n",
    "                #break\n",
    "            #print(inPath+file)\n",
    "            if(file[-4:]==\".xml\"):\n",
    "                clear_output(wait=True)\n",
    "                print(\"Working for: (\"+str(count)+\"/\"+str(totalFiles)+\")\"+ss+\", \"+ctype+\", \"+file)    \n",
    "                try:\n",
    "                    tree = ET.parse(inPath+file)\n",
    "                    root = tree.getroot()\n",
    "                    patterns=[]\n",
    "                    for n in root:\n",
    "                        print_path(n)\n",
    "\n",
    "                    file_tokens = []\n",
    "                    for p in patterns:\n",
    "                        for t in p.split('-'):\n",
    "                            if(len(t)>0):\n",
    "                                file_tokens.append(t)\n",
    "\n",
    "                    list_ts = []\n",
    "                    for ngram in [2, 3]: #n-gram = 2 & 3\n",
    "                        temp=zip(*[file_tokens[i:] for i in range(0, ngram)]) \n",
    "                        list_ts +=[' '.join(ngram) for ngram in temp]\n",
    "                    \n",
    "                    countedValues = Counter(list_ts)\n",
    "                    \n",
    "                    result_row = [file.split('.')[0]] #Take only the commit name from the file\n",
    "                    ZeroLine = 0 #Checks if all the values in this row is zero\n",
    "                    for lp in list_ts_features:\n",
    "                        if(countedValues[lp] > 0):\n",
    "                            ZeroLine += 1\n",
    "                            result_row.append(countedValues[lp]) \n",
    "                        else: \n",
    "                            result_row.append(0)\n",
    "                    \n",
    "                    if(ZeroLine > 0):\n",
    "                        dataset.append(result_row)\n",
    "\n",
    "                except: not_worked += ss+\", \"+ctype+\", \"+file+\"\\n\"\n",
    "                    \n",
    "            count+=1\n",
    "                \n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "205dc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset).to_csv(ss+'_ts_dataset.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
